// Default to /tmp directory if $VSC_SCRATCH scratch env is not available,
// see: https://github.com/nf-core/configs?tab=readme-ov-file#adding-a-new-config
def scratch_dir   = System.getenv("VSC_SCRATCH") ?: "/tmp"
def tier1_project = System.getenv("SLURM_ACCOUNT") ?: null

// Perform work directory cleanup when the run has succesfully completed
// cleanup = true

// Get the hostname and check some values for tier1
def hostname = "genius"
try {
    hostname = ['/bin/bash', '-c', 'sinfo --clusters=genius,wice -s | head -n 1'].execute().text.replace('CLUSTER: ','')
} catch (java.io.IOException e) {
    System.err.println("WARNING: Could not run sinfo to determine current cluster, defaulting to genius")
}

if (! tier1_project && (hostname.contains("genius") || hostname.contains("wice"))) {
    System.err.println("Please specify your VSC project account with environment variable SLURM_ACCOUNT.")
    System.exit(1)
}

// Reduce the job submit rate to about 50 per minute, this way the server won't be bombarded with jobs
// Limit queueSize to keep job rate under control and avoid timeouts
executor {
    submitRateLimit = '50/1min'
    queueSize = 30
    exitReadTimeout = "10min"
}

// Add backoff strategy to catch cluster timeouts and proper symlinks of files in scratch to the work directory
process {
    executor      = 'slurm'
    scratch       = "$scratch_dir"
    stageInMode   = "symlink"
    stageOutMode  = "rsync"
    errorStrategy = { sleep(Math.pow(2, task.attempt) * 200 as long); return 'retry' }
    maxRetries    = 3
    array         = 50
}

// Specify that singularity should be used and where the cache dir will be for the images
singularity {
    enabled = true
    autoMounts = true
    cacheDir = "$scratch_dir/.singularity"
}

params {
    config_profile_contact     = 'GitHub: @Joon-Klaps - Email: joon.klaps@kuleuven.be'
    config_profile_url         = 'https://docs.vscentrum.be/en/latest/index.html'
}

env {
    APPTAINER_TMPDIR="$scratch_dir/.apptainer/tmp"
    APPTAINER_CACHEDIR="$scratch_dir/.apptainer/cache"
}

// AWS maximum retries for errors (This way the pipeline doesn't fail if the download fails one time)
aws {
    maxErrorRetry = 3
}

// Define profiles for each cluster
profiles {
    genius {
        params.config_profile_description = 'genius profile for use on the genius cluster of the VSC HPC.'

        process {
            clusterOptions = { "--clusters=genius --account=$tier1_project" }
            resourceLimits = [
                memory: 703.GB, // 768 - 65 so 65GB for overhead, max is 720000MB
                cpus: 36,
                time: 168.h
            ]

            queue = {
                switch (task.memory) {
                case { it >=  175.GB }: // max is 180000
                    switch (task.time) {
                    case { it >= 72.h }:
                        return 'dedicated_big_bigmem,dedicated_big_batch,bigmem_long'
                    default:
                        return 'bigmem'
                    }
                default:
                    switch (task.time) {
                    case { it >= 72.h }:
                        return 'batch_long'
                    default:
                        return 'batch'
                    }
                }
            }

        }
    }

    genius_gpu {

        params.config_profile_description = 'genius_gpu profile for use on the genius cluster of the VSC HPC.'

        docker.runOptions       = '-u $(id -u):$(id -g) --gpus all'
        apptainer.runOptions    = '--containall --cleanenv --nv'
        singularity.runOptions  = '--containall --cleanenv --nv'

        process {
            beforeScript   = 'module load cuDNN/8.4.1.50-CUDA-11.7.0'
            clusterOptions = { "--clusters=genius --account=$tier1_project" }

            resourceLimits = [
                memory: 703.GB, // 768 - 65 so 65GB for overhead, max is 720000MB
                cpus: 36,
                time: 168.h,
            ]

            queue = {
                switch (task.memory) {
                case { it >=  175.GB }: // max is 180000
                    switch (task.time) {
                    case { it >= 72.h }:
                        return 'gpu_v100_long'
                    default:
                        return 'gpu_v100'
                    }
                default:
                    switch (task.time) {
                    case { it >= 72.h }:
                        return 'gpu_p100_long,amd_long'
                    default:
                        return 'gpu_p100,gpu_p100_debug,amd'
                    }
                }
            }

        }
    }

    wice {
        params.config_profile_description = 'wice profile for use on the Wice cluster of the VSC HPC.'

        process {
            clusterOptions = { "--clusters=wice --account=$tier1_project"}
            resourceLimits = [
                memory: 1968.GB, // max is 2016000
                cpus: 72,
                time: 168.h
            ]

            queue = {
                switch (task.memory) {
                case { it >=  239.GB }:  // max is 244800
                    switch (task.time) {
                    case { it >= 72.h }:
                        return 'dedicated_big_bigmem'
                    default:
                        return 'bigmem,hugemem'
                    }
                default:
                    switch (task.time) {
                    case { it >= 72.h }:
                        return 'batch_long,batch_icelake_long,batch_sapphirerapids_long'
                    default:
                        return 'batch,batch_sapphirerapids,batch_icelake'
                    }
                }
            }

        }
    }

    wice_gpu {

        params.config_profile_description = 'wice_gpu profile for use on the genius cluster of the VSC HPC.'

        docker.runOptions       = '-u $(id -u):$(id -g) --gpus all'
        apptainer.runOptions    = '--containall --cleanenv --nv'
        singularity.runOptions  = '--containall --cleanenv --nv'

        process {
            beforeScript   = 'module load cuDNN/8.4.1.50-CUDA-11.7.0'
            clusterOptions = { "--clusters=genius --account=$tier1_project" }
            resourceLimits = [
                memory: 703.GB, // 768 - 65 so 65GB for overhead, max is 720000MB
                cpus: 60,
                time: 168.h
            ]

            queue = {
                switch (task.memory) {
                case { it >=  478.GB }:  // max is 489600
                    switch (task.time) {
                    case { it >= 72.h }:
                        return 'dedicated_big_gpu_h100,dedicated_big_gpu'
                    default:
                        return 'gpu,gpu_h100'
                    }
                default:
                    switch (task.time) {
                    case { it >= 72.h }:
                        return 'gpu_a100'
                    default:
                        return 'gpu_a100_debug'
                    }
                }
            }

        }
    }

    superdome {
        params.config_profile_description = 'superdome profile for use on the genius cluster of the VSC HPC.'

        process {
            clusterOptions = {"--clusters=genius --account=$tier1_project"}
            resourceLimits = [
                memory: 5772.GB, // 6000 - 228 so 228GB for overhead, max is 5910888MB
                cpus: 14,
                time: 168.h
            ]

            queue = { task.time <= 72.h ? 'superdome' : 'superdome_long' }
        }
    }
}


