// Default to /tmp directory if $VSC_SCRATCH scratch env is not available,
// see: https://github.com/nf-core/configs?tab=readme-ov-file#adding-a-new-config
scratch_dir   = System.getenv("VSC_SCRATCH") ?: "/tmp"
tier1_project = System.getenv("SLURM_ACCOUNT") ?: null
avail_queues  = System.getenv("VSC_DEDICATED_QUEUES") ?: null
def availQueues = avail_queues?.toString()?.split(',')

// Perform work directory cleanup when the run has succesfully completed
// cleanup = true

// Reduce the job submit rate to about 50 per minute, this way the server won't be bombarded with jobs
// Limit queueSize to keep job rate under control and avoid timeouts
executor {
    submitRateLimit = '50/1min'
    queueSize = 50
    exitReadTimeout = "10min"
}

// Add backoff strategy to catch cluster timeouts and proper symlinks of files in scratch to the work directory
process {
    executor      = 'slurm'
    stageInMode   = "symlink"
    stageOutMode  = "rsync"
    errorStrategy = { sleep(Math.pow(2, task.attempt ?: 1) * 200 as long); return 'retry' }
    maxRetries    = 3
    array         = 30
}

// Specify that singularity should be used and where the cache dir will be for the images
singularity {
    enabled     = true
    autoMounts  = true
    cacheDir    = "$scratch_dir/.singularity"
    pullTimeout = "30 min"
}

params {
    config_profile_contact     = 'GitHub: @Joon-Klaps - Email: joon.klaps@kuleuven.be'
    config_profile_url         = 'https://docs.vscentrum.be/en/latest/index.html'
}

env {
    APPTAINER_TMPDIR="$scratch_dir/.apptainer/tmp"
    APPTAINER_CACHEDIR="$scratch_dir/.apptainer/cache"
}

// AWS maximum retries for errors (This way the pipeline doesn't fail if the download fails one time)
aws {
    maxErrorRetry = 3
}

// Function to limit task time when dedicated queues are not available
def limitTaskTime(time, maxTime) {
    return time > maxTime ? maxTime : time
}

// Define profiles for each cluster
profiles {
    genius {
        params.config_profile_description = 'genius profile for use on the genius cluster of the VSC HPC.'

        process {
            // 768 - 65 so 65GB for overhead, max is 720000MB
            resourceLimits = [ memory: 703.GB, cpus: 36, time: 168.h ]
            beforeScript = 'module load cluster/genius'
            clusterOptions = { "--clusters=genius --account=$tier1_project" }

            queue = {
                task.memory >= 175.GB ?
                    (task.time >= 72.h ? 'dedicated_big_bigmem,dedicated_big_batch,bigmem_long' : 'bigmem') :
                    (task.time >= 72.h ? 'batch_long' : 'batch')
            }

            withLabel: '.*gpu.*'{
                resourceLimits         = [ memory: 703.GB, cpus: 36 , time: 168.h ]
                apptainer.runOptions   = '--containall --cleanenv --nv'
                singularity.runOptions = '--containall --cleanenv --nv'

                // Set clusteroptions
                clusterOptions         = {
                    // suggested to use 9 cpus per gpu
                    def gpus = task.accelerator?.request ?: Math.max(1, Math.floor((task.cpus ?:1)/9) as int)
                    "--gres=gpu:${gpus} --clusters=genius --account=$tier1_project"
                }

                queue = {
                    task.memory >= 175.GB ?
                        (task.time >= 72.h ? 'gpu_v100_long' : 'gpu_v100') :
                        (task.time >= 72.h ? 'gpu_p100_long,amd_long' : 'gpu_p100,amd')
                }
            }
        }
    }

    genius_gpu {
        params.config_profile_description = 'genius_gpu profile for use on the genius cluster of the VSC HPC.'
        apptainer.runOptions              = '--containall --cleanenv --nv'
        singularity.runOptions            = '--containall --cleanenv --nv'

        process {
            // 768 - 65 so 65GB for overhead, max is 720000MB
            resourceLimits = [ memory: 703.GB, cpus: 36, time: 168.h]
            beforeScript   = 'module load cluster/genius'
            clusterOptions = {
                def gpus = task.accelerator?.request ?: Math.max(1, Math.floor((task.cpus ?:1)/9) as int)
                "--gres=gpu:${gpus} --clusters=genius --account=$tier1_project"
            }

            queue = {
                    task.memory >= 175.GB ?
                        (task.time >= 72.h ? 'gpu_v100_long' : 'gpu_v100') :
                        (task.time >= 72.h ? 'gpu_p100_long,amd_long' : 'gpu_p100,amd')
            }
        }
    }

    wice {
        params.config_profile_description = 'wice profile for use on the Wice cluster of the VSC HPC.'

        process {
            // max is 2016000
            resourceLimits = [ memory: 1968.GB, cpus: 72, time: 168.h ]
            beforeScript   = 'module load cluster/wice'

            // Set queue
            // The task time is limites to 72 hours if the memory is larger than 239GB
            // and dedicated queues are not available
            queue = {
                def maxTime = 72.h
                if (task.memory >= 239.GB) {
                    task.time = task.time >= maxTime && !availQueues.contains('dedicated_big_bigmem') ?
                        limitTaskTime(task.time, maxTime) : task.time
                    return availQueues.contains('dedicated_big_bigmem') ? 'dedicated_big_bigmem' : 'bigmem,hugemem'
                } else {
                    return task.time >= maxTime ? 'batch_long,batch_icelake_long,batch_sapphirerapids_long' : 'batch,batch_sapphirerapids,batch_icelake'
                }
            }

            // Set clusterOptions, changing account based on queue
            clusterOptions = {
                def queueValue = {
                    task.memory >= 239.GB ?
                        (task.time >= 72.h && availQueues.contains('dedicated_big_bigmem') ? 'dedicated_big_bigmem' : 'bigmem,hugemem') :
                        (task.time >= 72.h ? 'batch_long,batch_icelake_long,batch_sapphirerapids_long' : 'batch,batch_sapphirerapids,batch_icelake')
                }
                queueValue() =~ /dedicated/ ? "--clusters=wice --account=lp_big_wice_cpu" : "--clusters=wice --account=$tier1_project"
            }

            withLabel: '.*gpu.*' {
                resourceLimits         = [ memory: 703.GB, cpus: 64, time: 168.h ]
                apptainer.runOptions   = '--containall --cleanenv --nv'
                singularity.runOptions = '--containall --cleanenv --nv'

                // Set queue
                // The task time is limites to 72 hours if the memory is larger than 239GB
                // and dedicated queues are not available
                queue = {
                    def maxTime = 72.h
                    if (task.memory >= 239.GB) {
                        task.time = task.time >= maxTime && !availQueues.contains('dedicated_big_gpu_h100') ?
                            limitTaskTime(task.time, maxTime) : task.time
                        return availQueues.contains('dedicated_big_gpu_h100') ? 'dedicated_big_gpu_h100' : 'gpu_h100'
                    } else {
                        task.time = task.time >= maxTime && !availQueues.contains('dedicated_big_gpu') ?
                            limitTaskTime(task.time, maxTime) : task.time
                        return availQueues.contains('dedicated_big_gpu') ? 'dedicated_big_gpu' : 'gpu_a100,gpu'
                    }
                }

                clusterOptions = {
                    // suggested to use 16 cpus per gpu
                    def gpus = task.accelerator?.request ?: Math.max(1, Math.floor((task.cpus ?:1)/16) as int)
                    // Do same queue evaluation as above
                    def queueValue = {
                        task.memory >= 239.GB ?
                            (task.time >= 72.h && availQueues.contains('dedicated_big_gpu_h100') ? 'dedicated_big_gpu_h100' : 'gpu_h100') :
                            (task.time >= 72.h && availQueues.contains('dedicated_big_gpu') ? 'dedicated_big_gpu' : 'gpu_a100,gpu')
                    }

                    // Set clusterOptions, changing account based on queue
                    queueValue() =~ /dedicated_big_gpu_h100/ ? "--clusters=wice --account=lp_big_wice_gpu_h100 --gres=gpu:${gpus}" :
                    queueValue() =~ /dedicated_big_gpu/ ? "--clusters=wice --account=lp_big_wice_gpu --gres=gpu:${gpus}" :
                    "--clusters=wice --account=$tier1_project --gres=gpu:${gpus}"
                }
            }
        }
    }

    wice_gpu {
        params.config_profile_description = 'wice_gpu profile for use on the Wice cluster of the VSC HPC.'
        apptainer.runOptions              = '--containall --cleanenv --nv'
        singularity.runOptions            = '--containall --cleanenv --nv'

        process {
            // 768 - 65 so 65GB for overhead, max is 720000MB
            resourceLimits = [ memory: 703.GB, cpus: 64, time: 168.h ]
            beforeScript   = 'module load cluster/wice'
                // Set queue
                // The task time is limites to 72 hours if the memory is larger than 239GB
                // and dedicated queues are not available
            queue = {
                    def maxTime = 72.h
                    if (task.memory >= 239.GB) {
                        task.time = task.time >= maxTime && !availQueues.contains('dedicated_big_gpu_h100') ?
                            limitTaskTime(task.time, maxTime) : task.time
                        return availQueues.contains('dedicated_big_gpu_h100') ? 'dedicated_big_gpu_h100' : 'gpu_h100'
                    } else {
                        task.time = task.time >= maxTime && !availQueues.contains('dedicated_big_gpu') ?
                            limitTaskTime(task.time, maxTime) : task.time
                        return availQueues.contains('dedicated_big_gpu') ? 'dedicated_big_gpu' : 'gpu_a100,gpu'
                    }
            }

            // Set clusteroptions
            clusterOptions = {
                // suggested to use 16 cpus per gpu
                def gpus = task.accelerator?.request ?: Math.max(1, Math.floor((task.cpus ?:1)/16) as int)
                // Do same queue evaluation as above, without adjusting task.time
                def queueValue = {
                    task.memory >= 239.GB ?
                        (task.time >= 72.h && availQueues.contains('dedicated_big_gpu_h100') ? 'dedicated_big_gpu_h100' : 'gpu_h100') :
                        (task.time >= 72.h && availQueues.contains('dedicated_big_gpu') ? 'dedicated_big_gpu' : 'gpu_a100,gpu')
                }

                // Set clusterOptions, changing account based on queue
                queueValue() =~ /dedicated_big_gpu_h100/ ? "--clusters=wice --account=lp_big_wice_gpu_h100 --gres=gpu:${gpus}" :
                queueValue() =~ /dedicated_big_gpu/ ? "--clusters=wice --account=lp_big_wice_gpu --gres=gpu:${gpus}" :
                "--clusters=wice --account=$tier1_project --gres=gpu:${gpus}"
            }
        }
    }

    superdome {
        params.config_profile_description = 'superdome profile for use on the genius cluster of the VSC HPC.'

        process {
            clusterOptions = {"--clusters=genius --account=$tier1_project"}
            beforeScript   = 'module load cluster/genius/superdome'
            // 6000 - 228 so 228GB for overhead, max is 5910888MB
            resourceLimits = [ memory: 5772.GB, cpus: 14, time: 168.h]

            queue = { task.time <= 72.h ? 'superdome' : 'superdome_long' }
        }
    }
}


